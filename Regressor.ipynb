{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGAmFtN6i3fZ",
        "outputId": "66d5ed74-a410-48d1-9026-3694813e6c96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vMZr83xDjKGc",
        "outputId": "bd00c0fa-456c-46f3-8fe6-4f5946fe5636"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " CSV created with 407 entries at: /content/drive/MyDrive/arch/leaf_scores.csv\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Define the base image directory\n",
        "base_dir = '/content/drive/MyDrive/arch/wheat_leaf (1)/'\n",
        "\n",
        "# Folder to score mapping\n",
        "folder_score_map = {\n",
        "    'Healthy': 10,\n",
        "    'stripe_rust': 4,\n",
        "    'septoria': 6\n",
        "}\n",
        "\n",
        "# Collect image paths and scores\n",
        "data = []\n",
        "for folder, score in folder_score_map.items():\n",
        "    folder_path = os.path.join(base_dir, folder)\n",
        "    for fname in os.listdir(folder_path):\n",
        "        if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "            relative_path = os.path.join(folder, fname)\n",
        "            data.append({'image_path': relative_path, 'score': score})\n",
        "\n",
        "# Create dataframe\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Sort by filename\n",
        "df = df.sort_values(by='image_path')\n",
        "\n",
        "# Save to CSV\n",
        "csv_path = '/content/drive/MyDrive/arch/leaf_scores.csv'\n",
        "df.to_csv(csv_path, index=False)\n",
        "\n",
        "print(f\" CSV created with {len(df)} entries at: {csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Kv--Vngsjrjv"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "from torchvision import transforms\n",
        "\n",
        "class LeafHealthScoreDataset(Dataset):\n",
        "    def __init__(self, csv_file, image_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        img_path = os.path.join(self.image_dir, row['image_path'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        score = torch.tensor(row['score'], dtype=torch.float32)\n",
        "        return image, score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BBscJuZBj1B9"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "csv_path = '/content/drive/MyDrive/arch/leaf_scores.csv'\n",
        "image_dir = '/content/drive/MyDrive/arch/wheat_leaf (1)'\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "full_dataset = LeafHealthScoreDataset(csv_path, image_dir, transform=transform)\n",
        "\n",
        "# Split 80/20\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=16)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HUc9YHcyj5QP"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResNetRegressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.base_model = models.resnet50(pretrained=True)\n",
        "        self.base_model.fc = nn.Linear(self.base_model.fc.in_features, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.base_model(x).squeeze(1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "194CQiWgkDAC",
        "outputId": "4c00420b-6364-4e02-a1c0-dada48a72b7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 107MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Train Loss: 25.7294, Val Loss: 33.5909, Val MAE: 5.4737\n",
            "Epoch 2, Train Loss: 5.0072, Val Loss: 3.4530, Val MAE: 1.6065\n",
            "Epoch 3, Train Loss: 1.3973, Val Loss: 1.1852, Val MAE: 0.8287\n",
            "Epoch 4, Train Loss: 0.9491, Val Loss: 1.6604, Val MAE: 1.0174\n",
            "Epoch 5, Train Loss: 0.5285, Val Loss: 0.7241, Val MAE: 0.5419\n",
            "Epoch 6, Train Loss: 0.5544, Val Loss: 0.5082, Val MAE: 0.5681\n",
            "Epoch 7, Train Loss: 0.4807, Val Loss: 0.4819, Val MAE: 0.5421\n",
            "Epoch 8, Train Loss: 0.9620, Val Loss: 0.2518, Val MAE: 0.3678\n",
            "Epoch 9, Train Loss: 0.5730, Val Loss: 0.3581, Val MAE: 0.4146\n",
            "Epoch 10, Train Loss: 0.6868, Val Loss: 0.3324, Val MAE: 0.4491\n"
          ]
        }
      ],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import optim\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = ResNetRegressor().to(device)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, scores in train_loader:\n",
        "        images, scores = images.to(device), scores.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, scores)   # MSELoss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    val_mae = 0\n",
        "    with torch.no_grad():\n",
        "        for images, scores in val_loader:\n",
        "            images, scores = images.to(device), scores.to(device)\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, scores)   # MSE\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            mae = torch.mean(torch.abs(outputs - scores))   # MAE\n",
        "            val_mae += mae.item()\n",
        "\n",
        "    avg_val_loss = val_loss / len(val_loader)\n",
        "    avg_val_mae = val_mae / len(val_loader)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}, Train Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val MAE: {avg_val_mae:.4f}\")\n",
        "\n",
        "torch.save(model.state_dict(), \"/content/drive/MyDrive/arch/resnet50_wheat.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEBEG3Qvm9xI"
      },
      "source": [
        "predictions on unseen data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "nI7hkYhmnAdT"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XkQ3wlNunDBc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "def predict_leaf_health(model, image_path, transform, device):\n",
        "    model.eval()\n",
        "    image = Image.open(image_path).convert('RGB')\n",
        "    input_tensor = transform(image).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(input_tensor)\n",
        "        score = output.item()\n",
        "    return round(score, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIjYiidOnJIL",
        "outputId": "40c76475-c1d7-467a-d0cd-5bba86463fea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brown_rust009.jpg              ➜ Predicted Score: 6.08\n",
            "Brown_rust029.jpg              ➜ Predicted Score: 7.16\n",
            "Brown_rust040.jpg              ➜ Predicted Score: 8.98\n",
            "Brown_rust007.jpg              ➜ Predicted Score: 7.71\n",
            "Brown_rust018.jpg              ➜ Predicted Score: 6.65\n",
            "Brown_rust025.jpg              ➜ Predicted Score: 9.25\n",
            "Brown_rust023.jpg              ➜ Predicted Score: 9.14\n",
            "Brown_rust016.jpg              ➜ Predicted Score: 8.53\n",
            "Brown_rust001.jpg              ➜ Predicted Score: 6.95\n",
            "Brown_rust038.jpg              ➜ Predicted Score: 8.52\n",
            "Brown_rust005.jpg              ➜ Predicted Score: 8.69\n",
            "Brown_rust004.jpg              ➜ Predicted Score: 8.82\n",
            "Brown_rust006.jpg              ➜ Predicted Score: 7.82\n",
            "Brown_rust015.jpg              ➜ Predicted Score: 8.9\n",
            "Brown_rust010.jpg              ➜ Predicted Score: 9.13\n",
            "Brown_rust024.jpg              ➜ Predicted Score: 7.81\n",
            "Brown_rust021.jpg              ➜ Predicted Score: 5.86\n",
            "Brown_rust030.jpg              ➜ Predicted Score: 9.21\n",
            "Brown_rust008.jpg              ➜ Predicted Score: 7.08\n",
            "Brown_rust028.jpg              ➜ Predicted Score: 6.89\n",
            "Brown_rust013.jpg              ➜ Predicted Score: 6.67\n",
            "Brown_rust014.jpg              ➜ Predicted Score: 9.3\n",
            "Brown_rust027.jpg              ➜ Predicted Score: 6.94\n",
            "Brown_rust011.jpg              ➜ Predicted Score: 8.31\n",
            "Brown_rust012.jpg              ➜ Predicted Score: 8.28\n",
            "Brown_rust034.jpg              ➜ Predicted Score: 9.37\n",
            "Brown_rust031.jpg              ➜ Predicted Score: 9.0\n",
            "Brown_rust033.jpg              ➜ Predicted Score: 10.64\n",
            "Brown_rust020.jpg              ➜ Predicted Score: 8.35\n",
            "Brown_rust043.jpg              ➜ Predicted Score: 9.23\n",
            "Brown_rust032.jpg              ➜ Predicted Score: 7.14\n",
            "Brown_rust019.jpg              ➜ Predicted Score: 8.67\n",
            "Brown_rust002.jpg              ➜ Predicted Score: 8.07\n",
            "Brown_rust035.jpg              ➜ Predicted Score: 10.14\n",
            "Brown_rust041.jpg              ➜ Predicted Score: 8.63\n",
            "Brown_rust042.jpg              ➜ Predicted Score: 8.85\n",
            "Healthy023.jpg                 ➜ Predicted Score: 8.86\n",
            "Healthy011.jpg                 ➜ Predicted Score: 9.31\n",
            "Healthy018.jpg                 ➜ Predicted Score: 9.2\n",
            "Healthy029.jpg                 ➜ Predicted Score: 9.54\n",
            "Healthy013.jpg                 ➜ Predicted Score: 10.15\n",
            "Healthy034.jpg                 ➜ Predicted Score: 7.18\n",
            "Healthy002.jpg                 ➜ Predicted Score: 9.17\n",
            "Healthy004.jpg                 ➜ Predicted Score: 8.85\n",
            "Healthy035.jpg                 ➜ Predicted Score: 9.31\n",
            "Healthy024.jpg                 ➜ Predicted Score: 8.61\n",
            "Healthy012.jpg                 ➜ Predicted Score: 8.49\n",
            "Healthy003.jpg                 ➜ Predicted Score: 10.1\n",
            "Healthy009.jpg                 ➜ Predicted Score: 9.42\n",
            "Healthy031.jpg                 ➜ Predicted Score: 9.24\n",
            "Healthy021.jpg                 ➜ Predicted Score: 9.69\n",
            "Healthy020.jpg                 ➜ Predicted Score: 8.88\n",
            "Healthy016.jpg                 ➜ Predicted Score: 9.39\n",
            "Healthy005.jpg                 ➜ Predicted Score: 8.72\n",
            "Healthy017.jpg                 ➜ Predicted Score: 9.52\n",
            "Healthy007.jpg                 ➜ Predicted Score: 9.75\n",
            "Healthy008.jpg                 ➜ Predicted Score: 9.63\n",
            "Healthy019.jpg                 ➜ Predicted Score: 7.44\n",
            "Healthy010.jpg                 ➜ Predicted Score: 8.8\n",
            "Healthy014.jpg                 ➜ Predicted Score: 9.53\n",
            "Healthy022.jpg                 ➜ Predicted Score: 10.2\n",
            "Healthy030.jpg                 ➜ Predicted Score: 9.02\n",
            "Healthy038.jpg                 ➜ Predicted Score: 7.54\n",
            "Healthy028.jpg                 ➜ Predicted Score: 9.4\n",
            "Yellow_rust004.jpg             ➜ Predicted Score: 6.88\n",
            "Yellow_rust020.jpg             ➜ Predicted Score: 8.0\n",
            "Yellow_rust031.jpg             ➜ Predicted Score: 7.99\n",
            "Yellow_rust023.jpg             ➜ Predicted Score: 6.98\n",
            "Yellow_rust028.jpg             ➜ Predicted Score: 8.45\n",
            "Yellow_rust011.jpg             ➜ Predicted Score: 9.75\n",
            "Yellow_rust001.jpg             ➜ Predicted Score: 7.06\n",
            "Yellow_rust010.jpg             ➜ Predicted Score: 6.76\n",
            "Yellow_rust021.jpg             ➜ Predicted Score: 7.12\n",
            "Yellow_rust007.jpg             ➜ Predicted Score: 7.13\n",
            "Yellow_rust032.jpg             ➜ Predicted Score: 8.03\n",
            "Yellow_rust025.jpg             ➜ Predicted Score: 6.93\n",
            "Yellow_rust030.jpg             ➜ Predicted Score: 7.95\n",
            "Yellow_rust016.jpg             ➜ Predicted Score: 7.91\n",
            "Yellow_rust022.jpg             ➜ Predicted Score: 8.66\n",
            "Yellow_rust008.jpg             ➜ Predicted Score: 7.24\n",
            "Yellow_rust005.jpg             ➜ Predicted Score: 7.11\n",
            "Yellow_rust012.jpg             ➜ Predicted Score: 6.59\n",
            "Yellow_rust014.jpg             ➜ Predicted Score: 7.92\n",
            "Yellow_rust003.jpg             ➜ Predicted Score: 7.52\n",
            "Yellow_rust009.jpg             ➜ Predicted Score: 8.38\n",
            "Yellow_rust026.jpg             ➜ Predicted Score: 7.45\n",
            "Yellow_rust002.jpg             ➜ Predicted Score: 6.73\n",
            "Yellow_rust013.jpg             ➜ Predicted Score: 7.88\n"
          ]
        }
      ],
      "source": [
        "\n",
        "unseen_folder = '/content/drive/MyDrive/arch/unseen_data'\n",
        "\n",
        "predictions = []\n",
        "\n",
        "for fname in os.listdir(unseen_folder):\n",
        "    if fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        full_path = os.path.join(unseen_folder, fname)\n",
        "        predicted_score = predict_leaf_health(model, full_path, transform, device)\n",
        "        predictions.append((fname, predicted_score))\n",
        "\n",
        "# Display results\n",
        "for fname, score in predictions:\n",
        "    print(f\"{fname:30s} ➜ Predicted Score: {score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBo5_YjLnNBk",
        "outputId": "1d2586ea-9253-46c2-b10b-51549aa319c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved predictions to predicted_scores.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_preds = pd.DataFrame(predictions, columns=['image_name', 'predicted_score'])\n",
        "df_preds.to_csv('/content/drive/MyDrive/arch/predicted_scores.csv', index=False)\n",
        "print(\"Saved predictions to predicted_scores.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91335z-uOmAe"
      },
      "source": [
        "second approach classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHBlls87OpFS",
        "outputId": "d6f2e956-6ce1-4d4d-ae3f-e9b133640697"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6IAciyhOt5M",
        "outputId": "67a46c0e-b086-4af9-93f2-16be0329a3a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "data_path = '/content/drive/MyDrive/arch/wheat_leaf (1)'\n",
        "\n",
        "# List subfolders\n",
        "print(\"Subfolders inside dataset directory:\")\n",
        "if os.path.exists(data_path):\n",
        "    for sub in os.listdir(data_path):\n",
        "        sub_path = os.path.join(data_path, sub)\n",
        "        if os.path.isdir(sub_path):\n",
        "            imgs = [f for f in os.listdir(sub_path) if f.lower().endswith(('.jpg','.jpeg','.png'))]\n",
        "            print(f\" - {sub}: {len(imgs)} images\")\n",
        "else:\n",
        "    print(\" Path does not exist:\", data_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aBylZsHbS3F_",
        "outputId": "28691f9e-5945-42d7-dbc6-8f930874ac36"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subfolders inside dataset directory:\n",
            " - Healthy: 102 images\n",
            " - septoria: 97 images\n",
            " - stripe_rust: 208 images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch, shutil\n",
        "\n",
        "data_path = Path('/content/drive/MyDrive/arch/wheat_leaf (1)')\n",
        "\n",
        "# 1. Detect empty class folders\n",
        "valid_exts = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff', '.webp'}\n",
        "empty_dirs = []\n",
        "for d in data_path.iterdir():\n",
        "    if d.is_dir():\n",
        "        n_imgs = sum(1 for p in d.rglob('*') if p.suffix.lower() in valid_exts)\n",
        "        if n_imgs == 0:\n",
        "            empty_dirs.append(d)\n",
        "\n",
        "# 2. Move empty folders aside so ImageFolder won’t see them\n",
        "if empty_dirs:\n",
        "    ignore_root = data_path.with_name(data_path.name + '_IGNORED_EMPTY')\n",
        "    ignore_root.mkdir(exist_ok=True)\n",
        "    for d in empty_dirs:\n",
        "        shutil.move(str(d), str(ignore_root / d.name))\n",
        "    print(f\"Moved empty class folders out of the dataset: {[d.name for d in empty_dirs]}\")\n",
        "else:\n",
        "    print(\"No empty class folders found.\")\n",
        "\n",
        "# 3. Now load dataset safely\n",
        "transform = transforms.Compose([transforms.Resize((224,224)), transforms.ToTensor()])\n",
        "full_dataset = datasets.ImageFolder(root=str(data_path), transform=transform)\n",
        "print(\"Classes:\", full_dataset.classes, \"Total images:\", len(full_dataset))\n",
        "\n",
        "# 4. Stratified split (fallback to random if needed)\n",
        "indices = list(range(len(full_dataset)))\n",
        "targets = full_dataset.targets\n",
        "try:\n",
        "    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42, stratify=targets)\n",
        "except ValueError:\n",
        "    train_idx, val_idx = train_test_split(indices, test_size=0.2, random_state=42, shuffle=True)\n",
        "\n",
        "train_ds = Subset(full_dataset, train_idx)\n",
        "val_ds   = Subset(full_dataset, val_idx)\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True,  num_workers=2, pin_memory=True)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, shuffle=False, num_workers=2, pin_memory=True)\n",
        "print(f\"Train/Val sizes: {len(train_ds)} / {len(val_ds)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RS-O3OyPGVjH",
        "outputId": "73f00052-d53a-428d-e548-d529b1995cc3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No empty class folders found.\n",
            "Classes: ['Healthy', 'septoria', 'stripe_rust'] Total images: 407\n",
            "Train/Val sizes: 325 / 82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WFi8LnDZOt-p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ce4e58-a91c-4a59-a8d9-344f06e8b1aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=2048, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Replaces last layer for 3 classes\n",
        "model.fc = nn.Linear(model.fc.in_features, 3)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Aeeh8Dm0OuBZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "89849ab5-ce36-4714-f0a2-74f2b4dafc15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Loss: 6.2187\n",
            "Epoch 2/5, Loss: 1.1753\n",
            "Epoch 3/5, Loss: 0.8077\n",
            "Epoch 4/5, Loss: 1.8961\n",
            "Epoch 5/5, Loss: 1.2020\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Jqbc9fEgOuEM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d6b8d6-dc8c-4e6b-cef7-e59568aa8efa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation Accuracy: 92.68%\n"
          ]
        }
      ],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in val_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "E5j4QjwUTh22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f6a2e62-384a-455c-d0bd-9980ae97ad81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 13/13 [02:59<00:00, 13.78s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Auto-labeled CSV created at: /content/drive/MyDrive/arch/leaf_scores_auto.csv\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from torchvision.datasets import ImageFolder\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Load the full dataset again (with image paths)\n",
        "full_dataset = ImageFolder(root=data_path, transform=transform)\n",
        "full_loader = DataLoader(full_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "# Reverse class mapping ( 0: 'Healthy', 1: 'septoria', 2: 'stripe_rust')\n",
        "class_to_label = {v: k for k, v in full_dataset.class_to_idx.items()}\n",
        "\n",
        "# Prepare rows for CSV\n",
        "csv_rows = []\n",
        "image_idx = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, _ in tqdm(full_loader):\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = model(inputs)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "        for i in range(inputs.size(0)):\n",
        "            img_path = full_dataset.samples[image_idx][0]\n",
        "            folder, fname = img_path.split('/')[-2], img_path.split('/')[-1]\n",
        "            pred_class = predicted[i].item()\n",
        "\n",
        "            # Disease score mapping\n",
        "            disease_score = {0: 10, 1: 3, 2: 2}[pred_class]\n",
        "\n",
        "            # Estimated scores\n",
        "            if pred_class == 0:  # Healthy\n",
        "                appearance = 9.5\n",
        "                growth = 9.0\n",
        "                weed = 10.0\n",
        "            elif pred_class == 1:  # Septoria\n",
        "                appearance = 5.0\n",
        "                growth = 6.0\n",
        "                weed = 9.5\n",
        "            else:  # Stripe Rust\n",
        "                appearance = 4.5\n",
        "                growth = 5.0\n",
        "                weed = 8.0\n",
        "\n",
        "            csv_rows.append({\n",
        "                'filename': f\"{folder}/{fname}\",\n",
        "                'class': pred_class,\n",
        "                'disease': disease_score,\n",
        "                'appearance': appearance,\n",
        "                'growth': growth,\n",
        "                'weed': weed\n",
        "            })\n",
        "\n",
        "            image_idx += 1\n",
        "\n",
        "# Save the new CSV\n",
        "output_csv = '/content/drive/MyDrive/arch/leaf_scores_auto.csv'\n",
        "df = pd.DataFrame(csv_rows)\n",
        "df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\" Auto-labeled CSV created at: {output_csv}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzSOt1bwUOCF"
      },
      "source": [
        "Multi-Indicator Health Score Regression Model\n",
        "We’ll build a model that takes a leaf image and predicts:\n",
        "\n",
        "disease score\n",
        "\n",
        "appearance score\n",
        "\n",
        "growth score\n",
        "\n",
        "weed score\n",
        "All in the 1–10 range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "U2FXO3wDUPC0"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "\n",
        "class LeafHealthScoreDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        image_path = os.path.join(self.image_dir, row['filename'])\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        scores = torch.tensor([\n",
        "            row['disease'], row['appearance'],\n",
        "            row['growth'], row['weed']\n",
        "        ], dtype=torch.float32)\n",
        "        return image, scores\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "OWASauLHUf4O"
      },
      "outputs": [],
      "source": [
        "# Paths\n",
        "csv_path = '/content/drive/MyDrive/arch/leaf_scores_auto.csv'\n",
        "image_dir = '/content/drive/MyDrive/arch/wheat_leaf'\n",
        "\n",
        "\n",
        "\n",
        "full_dataset = LeafHealthScoreDataset(csv_path, image_dir, transform)\n",
        "\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "dy9vEXYAZzwR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "\n",
        "# Updated dataset class that skips missing or unreadable images\n",
        "class LeafHealthScoreDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        image_path = os.path.join(self.image_dir, row['filename'])\n",
        "\n",
        "        try:\n",
        "            image = Image.open(image_path).convert('RGB')\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            scores = torch.tensor([\n",
        "                row['disease'], row['appearance'],\n",
        "                row['growth'], row['weed']\n",
        "            ], dtype=torch.float32)\n",
        "            return image, scores\n",
        "\n",
        "        except (FileNotFoundError, UnidentifiedImageError):\n",
        "            return self.__getitem__((idx + 1) % len(self))\n",
        "\n",
        "# Paths\n",
        "csv_path = '/content/drive/MyDrive/arch/leaf_scores_auto.csv'\n",
        "image_dir = '/content/drive/MyDrive/arch/wheat_leaf (1)'\n",
        "\n",
        "# Image transform\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor()\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "L7n1DW6iZz_S"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "\n",
        "full_dataset = LeafHealthScoreDataset(csv_path, image_dir, transform)\n",
        "\n",
        "# Split 80/20\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9Xo3nb4oZ0Bt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd7d7cd3-9870-4d9a-bcbb-852abbce4520"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 73.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "class MultiIndicatorRegressor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        base = models.resnet18(pretrained=True)\n",
        "        self.backbone = nn.Sequential(*list(base.children())[:-1])\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(base.fc.in_features, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 4)  # Predict 4 health scores\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.head(x)\n",
        "        return torch.sigmoid(x) * 9 + 1  # Scale to 1–10 range\n",
        "\n",
        "# Set up model and device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = MultiIndicatorRegressor().to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "yTC75fnbaLTH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "49b7afa1-0e83-46ed-ba02-899f54f39923"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3382097041.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m             )\n\u001b[0;32m--> 647\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    648\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    352\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    830\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    831\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training loop\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSygniqMaD5P"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "val_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in val_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "print(f\"Validation MSE: {val_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vG_PgD5Jrmq5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EiH7eaSKrnM9"
      },
      "source": [
        "GRADCAM to model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_tFqtjMrc_W"
      },
      "outputs": [],
      "source": [
        "# Store activations and gradients\n",
        "feature_maps = None\n",
        "gradients = None\n",
        "\n",
        "def save_activation_hook(module, input, output):\n",
        "    global feature_maps\n",
        "    feature_maps = output.detach()\n",
        "\n",
        "def save_gradient_hook(module, grad_input, grad_output):\n",
        "    global gradients\n",
        "    gradients = grad_output[0].detach()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Z2c1TKYrrUt"
      },
      "outputs": [],
      "source": [
        "# Hook into the last conv layer of ResNet18 (layer4)\n",
        "last_conv_layer = model.backbone[-1]  # layer4 in resnet18\n",
        "last_conv_layer.register_forward_hook(save_activation_hook)\n",
        "last_conv_layer.register_backward_hook(save_gradient_hook)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "437Ll9uKrrXA"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.transforms.functional import to_pil_image\n",
        "\n",
        "def generate_gradcam(input_tensor, target_index):\n",
        "    model.eval()\n",
        "    input_tensor = input_tensor.unsqueeze(0).to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(input_tensor)\n",
        "    score = output[0, target_index]\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    score.backward()\n",
        "\n",
        "    # Compute GradCAM\n",
        "    weights = torch.mean(gradients, dim=(2, 3), keepdim=True)  # Global average pooling\n",
        "    cam = torch.sum(weights * feature_maps, dim=1).squeeze()\n",
        "    cam = torch.relu(cam)\n",
        "\n",
        "    # Normalize\n",
        "    cam -= cam.min()\n",
        "    cam /= cam.max()\n",
        "\n",
        "    # Resize to input size\n",
        "    cam = cam.cpu().numpy()\n",
        "    cam = cv2.resize(cam, (224, 224))\n",
        "\n",
        "    # Convert to heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "    return heatmap\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6l7zCIerylA"
      },
      "outputs": [],
      "source": [
        "# Load a sample image from your dataset\n",
        "img, scores = full_dataset[0]  # Change index to test others\n",
        "input_tensor = img.unsqueeze(0).to(device)\n",
        "\n",
        "# Indicator names\n",
        "indicators = ['disease', 'appearance', 'growth', 'weed']\n",
        "\n",
        "# Generate and plot GradCAMs\n",
        "for i in range(4):  # for each score\n",
        "    heatmap = generate_gradcam(img, target_index=i)\n",
        "\n",
        "    # Original image as numpy\n",
        "    img_np = np.transpose(img.cpu().numpy(), (1, 2, 0))\n",
        "    img_np = img_np / img_np.max()\n",
        "\n",
        "    # Overlay heatmap\n",
        "    overlay = 0.4 * heatmap + 0.6 * img_np\n",
        "    overlay = np.clip(overlay, 0, 1)\n",
        "\n",
        "    # Show it\n",
        "    plt.figure(figsize=(5, 5))\n",
        "    plt.title(f\"GradCAM for: {indicators[i]} score\")\n",
        "    plt.imshow(overlay)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_labels = []\n",
        "all_preds = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in val_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)  # get class index\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy().tolist())\n",
        "        all_preds.extend(preds.cpu().numpy().tolist())\n",
        "\n",
        "print(\"Labels length:\", len(all_labels))\n",
        "print(\"Preds length:\", len(all_preds))\n"
      ],
      "metadata": {
        "id": "cK0np00yCggX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import random_split\n",
        "\n",
        "# Dataset sizes\n",
        "train_size = int(0.7 * len(full_dataset))\n",
        "val_size   = int(0.15 * len(full_dataset))\n",
        "test_size  = len(full_dataset) - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(full_dataset, [train_size, val_size, test_size])\n",
        "\n",
        "# Dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset, batch_size=32)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32)\n"
      ],
      "metadata": {
        "id": "9Q4KpPJKkMYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Step 1: Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "# Step 2: Loop through test loader\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:   # make sure test_loader is defined\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "\n",
        "        y_true.extend(labels.cpu().numpy())\n",
        "        y_pred.extend(preds.cpu().numpy())\n",
        "\n",
        "# Step 3: Define class names\n",
        "class_names = [\"Healthy\", \"Septoria\", \"Stripe Rust\"]\n",
        "\n",
        "# Step 4: Compute confusion matrix\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# Step 5: Plot\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix for Wheat Leaf Classification\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CTA7qVr3C8km"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "\n",
        "# Set which loader to evaluate on\n",
        "EVAL_LOADER = test_loader if 'test_loader' in globals() else val_loader\n",
        "\n",
        "# Optional: override class names if needed\n",
        "CLASS_NAMES = getattr(getattr(EVAL_LOADER, 'dataset', None), 'dataset', getattr(EVAL_LOADER, 'dataset', None))\n",
        "CLASS_NAMES = getattr(CLASS_NAMES, 'classes', None) or [\"Healthy\", \"Septoria\", \"Stripe Rust\"]\n",
        "\n",
        "\n",
        "def to_numpy(x):\n",
        "    if isinstance(x, torch.Tensor):\n",
        "        return x.detach().cpu().numpy()\n",
        "    return np.asarray(x)\n",
        "\n",
        "def coerce_to_class_indices(arr):\n",
        "    \"\"\"\n",
        "    Convert predictions/labels into 1-D integer class indices.\n",
        "    Handles:\n",
        "      - shape (N,) ints/floats -> rounds floats in {0,1} to binary\n",
        "      - shape (N,K) one-hot or probabilities -> argmax along axis=1\n",
        "    Raises if looks like regression (continuous values not in {0,1}).\n",
        "    \"\"\"\n",
        "    a = to_numpy(arr)\n",
        "\n",
        "    # Squeeze extra dimensions like (N,1) -> (N,)\n",
        "    while a.ndim > 2:\n",
        "        a = np.squeeze(a, axis=-1)\n",
        "\n",
        "    if a.ndim == 1:\n",
        "        if np.issubdtype(a.dtype, np.floating):\n",
        "            # If all values are in [0,1], treat as binary probs -> threshold at 0.5\n",
        "            if np.all((a >= 0.0) & (a <= 1.0)):\n",
        "                return (a >= 0.5).astype(int)\n",
        "            # If floats but not probs, likely regression scores -> error\n",
        "            raise ValueError(\n",
        "                \"Detected continuous 1-D targets/preds (regression-like). \"\n",
        "                \"For a confusion matrix, provide class indices or use binning.\"\n",
        "            )\n",
        "        return a.astype(int)\n",
        "\n",
        "    if a.ndim == 2:\n",
        "        # One-hot (rows sum≈1) or probabilities -> argmax\n",
        "        return np.argmax(a, axis=1).astype(int)\n",
        "\n",
        "    raise ValueError(f\"Unsupported array shape for classification targets/preds: {a.shape}\")\n",
        "\n",
        "# Device\n",
        "try:\n",
        "    device = next(model.parameters()).device\n",
        "except Exception:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "y_true_list, y_pred_list = [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in EVAL_LOADER:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "\n",
        "        # Expect classifier logits of shape [B, K]\n",
        "        if outputs.ndim == 2 and outputs.shape[1] > 1:\n",
        "            preds = outputs.argmax(dim=1)\n",
        "        elif outputs.ndim == 2 and outputs.shape[1] == 1:\n",
        "\n",
        "            raise ValueError(\n",
        "                f\"Model output shape {tuple(outputs.shape)} suggests a single-output head. \"\n",
        "                \"If binary classification, supply 2-class logits; if regression, bin predictions first.\"\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                f\"Unexpected model output shape {tuple(outputs.shape)}. Expected [B, K] for K classes.\"\n",
        "            )\n",
        "\n",
        "        y_true_list.append(labels.detach().cpu())\n",
        "        y_pred_list.append(preds.detach().cpu())\n",
        "\n",
        "# COERCE TO 1-D INT LABELS\n",
        "y_true = torch.cat(y_true_list, dim=0)\n",
        "y_pred = torch.cat(y_pred_list, dim=0)\n",
        "\n",
        "y_true = coerce_to_class_indices(y_true)\n",
        "y_pred = coerce_to_class_indices(y_pred)\n",
        "\n",
        "# Safety: enforce same label set and length match\n",
        "assert y_true.shape == y_pred.shape, f\"y_true and y_pred must have same shape; got {y_true.shape} vs {y_pred.shape}\"\n",
        "\n",
        "num_classes = int(max(y_true.max(), y_pred.max())) + 1 if y_true.size > 0 else len(CLASS_NAMES)\n",
        "if len(CLASS_NAMES) != num_classes:\n",
        "    CLASS_NAMES = [f\"Class {i}\" for i in range(num_classes)]\n",
        "\n",
        "# Confusion Matrix\n",
        "cm = confusion_matrix(y_true, y_pred, labels=np.arange(num_classes))\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "            xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uJ4QHVqbP16L",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 507
        },
        "outputId": "98d3605b-023a-4af2-b62c-78121d597169"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 600x500 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# FULL-SIZE GRAD-CAM FOR WHEAT LEAVES\n",
        "\n",
        "\n",
        "!pip install -q grad-cam opencv-python pillow\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/arch/wheat_leaf (1)\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/arch/resnet50_wheat.pth\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/arch/gradcam_full\"\n",
        "VAL_SPLIT = 0.2\n",
        "BATCH_SIZE = 16\n",
        "MAX_SAMPLES = 20   # how many Grad-CAM images to generate\n",
        "ALPHA = 0.35\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Transforms\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# Dataset & Loader\n",
        "full_ds = datasets.ImageFolder(root=DATA_ROOT, transform=val_tf)\n",
        "val_size = int(len(full_ds) * VAL_SPLIT)\n",
        "train_size = len(full_ds) - val_size\n",
        "_, val_ds = random_split(full_ds, [train_size, val_size])\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class_names = full_ds.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "\n",
        "# Load Model\n",
        "model = models.resnet50(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "model.load_state_dict(state_dict, strict=False)\n",
        "model = model.to(device).eval()\n",
        "\n",
        "# Target Layer for Grad-CAM\n",
        "target_layer = model.layer4[-1]\n",
        "\n",
        "# Helper: denormalize image\n",
        "def denormalize(img_tensor):\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = img_tensor.detach().cpu().permute(1, 2, 0).numpy()\n",
        "    img = (img * std + mean).clip(0, 1)\n",
        "    return (img * 255).astype(np.uint8)\n",
        "\n",
        "# Helper: mask leaf (suppress soil/bg)\n",
        "def leaf_mask_hsv(rgb):\n",
        "    hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
        "    H, S, V = hsv[:,:,0], hsv[:,:,1], hsv[:,:,2]\n",
        "    mask = (S > 40) & (V > 40)\n",
        "    mask = mask.astype(np.uint8) * 255\n",
        "    mask = cv2.medianBlur(mask, 5)\n",
        "    return mask > 0\n",
        "\n",
        "# Run Grad-CAM\n",
        "def run_gradcam(model, loader, class_names, out_dir, max_samples=20):\n",
        "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "    saved = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        preds = model(images).argmax(1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            if saved >= max_samples: break\n",
        "\n",
        "            input_img = images[i:i+1]\n",
        "            target = [ClassifierOutputTarget(int(preds[i]))]\n",
        "            grayscale_cam = cam(input_tensor=input_img, targets=target)[0]\n",
        "\n",
        "            # original image\n",
        "            orig = denormalize(images[i])\n",
        "            h, w = orig.shape[:2]\n",
        "\n",
        "            # resize CAM to full image size\n",
        "            cam_resized = cv2.resize(grayscale_cam, (w, h))\n",
        "            cam_resized = (cam_resized - cam_resized.min()) / (cam_resized.max() - cam_resized.min() + 1e-8)\n",
        "\n",
        "            # suppress background\n",
        "            mask = leaf_mask_hsv(orig)\n",
        "            cam_resized = cam_resized * mask.astype(np.float32)\n",
        "            if cam_resized.max() > 1e-6:\n",
        "                cam_resized /= cam_resized.max()\n",
        "\n",
        "            # overlay\n",
        "            heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
        "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "            overlay = cv2.addWeighted(orig, 0.75, heatmap, ALPHA, 0)\n",
        "\n",
        "            # save\n",
        "            pred_cls = class_names[int(preds[i])]\n",
        "            true_cls = class_names[int(labels[i])]\n",
        "            fn = Path(out_dir) / f\"{saved:03d}_pred-{pred_cls}_true-{true_cls}.jpg\"\n",
        "            Image.fromarray(overlay).save(fn)\n",
        "            saved += 1\n",
        "\n",
        "        if saved >= max_samples: break\n",
        "\n",
        "    print(f\" Saved {saved} Grad-CAM images to {out_dir}\")\n",
        "\n",
        "#  Execute\n",
        "run_gradcam(model, val_loader, class_names, OUT_DIR, max_samples=MAX_SAMPLES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rVFBnXc2KSgw",
        "outputId": "6ffd7ebe-71e2-46c9-fca8-f85f510eea32"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved 20 Grad-CAM images to /content/drive/MyDrive/arch/gradcam_full\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# GRAD-CAM WITH LEAF-ONLY HIGHLIGHTS (FULL IMAGE)\n",
        "\n",
        "\n",
        "!pip install -q grad-cam opencv-python pillow\n",
        "\n",
        "import os, cv2, numpy as np\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "import torch, torch.nn as nn\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms, models\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "\n",
        "DATA_ROOT = \"/content/drive/MyDrive/arch/wheat_leaf (1)\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/arch/resnet50_wheat.pth\"\n",
        "OUT_DIR = \"/content/drive/MyDrive/arch/gradcam_leaf_only\"\n",
        "VAL_SPLIT = 0.2\n",
        "BATCH_SIZE = 16\n",
        "MAX_SAMPLES = 20\n",
        "ALPHA = 0.35   # overlay transparency\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "#  TRANSFORMS\n",
        "val_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "# DATASET & LOADER\n",
        "full_ds = datasets.ImageFolder(root=DATA_ROOT, transform=val_tf)\n",
        "val_size = int(len(full_ds) * VAL_SPLIT)\n",
        "train_size = len(full_ds) - val_size\n",
        "_, val_ds = random_split(full_ds, [train_size, val_size])\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "class_names = full_ds.classes\n",
        "num_classes = len(class_names)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Build model with correct number of classes\n",
        "model = models.resnet50(weights=None)\n",
        "model.fc = nn.Linear(model.fc.in_features, len(full_ds.classes))\n",
        "\n",
        "# Load backbone weights, ignore fc mismatch\n",
        "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
        "new_state_dict = {k.replace(\"base_model.\", \"\"): v for k,v in state_dict.items() if \"fc\" not in k}\n",
        "model.load_state_dict(new_state_dict, strict=False)\n",
        "\n",
        "model = model.to(device).eval()\n",
        "\n",
        "\n",
        "#  GRAD-CAM TARGET LAYER\n",
        "target_layer = model.layer4[-1]\n",
        "\n",
        "#  HELPERS\n",
        "def denormalize(img_tensor):\n",
        "    \"\"\"Convert normalized tensor back to uint8 RGB image.\"\"\"\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = img_tensor.detach().cpu().permute(1,2,0).numpy()\n",
        "    img = (img * std + mean).clip(0,1)\n",
        "    return (img * 255).astype(np.uint8)\n",
        "\n",
        "def leaf_mask_hsv(rgb):\n",
        "    \"\"\"Strict HSV-based mask for leaf pixels only.\"\"\"\n",
        "    hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)\n",
        "    H, S, V = hsv[:,:,0], hsv[:,:,1], hsv[:,:,2]\n",
        "    mask = (S > 40) & (V > 40)    # remove dull background\n",
        "    mask = mask.astype(np.uint8) * 255\n",
        "    mask = cv2.medianBlur(mask, 5)\n",
        "    return mask > 0\n",
        "\n",
        "#  GRAD-CAM RUNNER\n",
        "def run_gradcam_leaf_only(model, loader, class_names, out_dir, max_samples=20):\n",
        "    cam = GradCAM(model=model, target_layers=[target_layer])\n",
        "    saved = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        preds = model(images).argmax(1)\n",
        "\n",
        "        for i in range(images.size(0)):\n",
        "            if saved >= max_samples: break\n",
        "\n",
        "            input_img = images[i:i+1]\n",
        "            target = [ClassifierOutputTarget(int(preds[i]))]\n",
        "            grayscale_cam = cam(input_tensor=input_img, targets=target)[0]\n",
        "\n",
        "            # recover original RGB\n",
        "            orig = denormalize(images[i])\n",
        "            h, w = orig.shape[:2]\n",
        "\n",
        "            # resize CAM to full image size\n",
        "            cam_resized = cv2.resize(grayscale_cam, (w, h))\n",
        "            cam_resized = (cam_resized - cam_resized.min()) / (cam_resized.max() - cam_resized.min() + 1e-8)\n",
        "\n",
        "            # force CAM only on leaf pixels\n",
        "            mask = leaf_mask_hsv(orig)\n",
        "            cam_resized = cam_resized * mask.astype(np.float32)\n",
        "            if cam_resized.max() > 1e-6:\n",
        "                cam_resized /= cam_resized.max()\n",
        "\n",
        "            # overlay CAM\n",
        "            heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
        "            heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "            overlay = cv2.addWeighted(orig, 0.75, heatmap, ALPHA, 0)\n",
        "\n",
        "            # save result\n",
        "            pred_cls = class_names[int(preds[i])]\n",
        "            true_cls = class_names[int(labels[i])]\n",
        "            fn = Path(out_dir) / f\"{saved:03d}_pred-{pred_cls}_true-{true_cls}.jpg\"\n",
        "            Image.fromarray(overlay).save(fn)\n",
        "            saved += 1\n",
        "\n",
        "        if saved >= max_samples: break\n",
        "\n",
        "    print(f\" Saved {saved} leaf-only Grad-CAM images to {out_dir}\")\n",
        "\n",
        "#  EXECUTE\n",
        "run_gradcam_leaf_only(model, val_loader, class_names, OUT_DIR, max_samples=MAX_SAMPLES)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKeFYaUrKh7G",
        "outputId": "b5caa200-2546-477c-c229-2de779d4debc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Saved 20 leaf-only Grad-CAM images to /content/drive/MyDrive/arch/gradcam_leaf_only\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cUQRJiYNJiA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}